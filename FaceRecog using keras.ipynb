{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <center> Neural Networks and Fuzzy Control EEE1007 </center>\n",
    "\n",
    "## <center> Face Recognition using Python and Keras (TensorFlow) </center>\n",
    "<br>\n",
    "\n",
    "<b> Rishu Sinha 15BEE0114  </b><br>\n",
    "<b> Anupam Bisht 15BEE0139 <br> </b>\n",
    "<b> Siddharth Bhargava 15BCE0628 </b>\n",
    "<br><br>\n",
    "<em>Professor Mathew M. Noel </em><br>\n",
    "**Slot**: G1\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset:\n",
    "\n",
    "We are using the yalefaces dataset, collected from the [this link](http://vismod.media.mit.edu/vismod/classes/mas622-00/datasets/YALE.tar.gz \"Yale Face Database Download\"). <br>\n",
    "TO know more about the yale faces database, you can read up at [this link](http://vision.ucsd.edu/content/yale-face-database \"Yale Face Database\").<br>\n",
    "To know more about face recognition databases, you can read up at [this link](http://www.face-rec.org/databases/ \"Face Recognition Databases\"). A lot of different face databases are available here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DatasetPath = []\n",
    "for i in os.listdir(\"yalefaces\"):\n",
    "    DatasetPath.append(os.path.join(\"yalefaces\", i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the dataset in a list along with their respective labels\n",
    "\n",
    "We use skimage library to read the images from the folder and then convert them to their grayscale version. The labels have been extracted from the filepath of each image using os library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imageData = []\n",
    "imageLabels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in DatasetPath:\n",
    "    imgRead = io.imread(i,as_grey=True)\n",
    "    imageData.append(imgRead)\n",
    "    \n",
    "    labelRead = int(os.path.split(i)[1].split(\".\")[0].replace(\"subject\", \"\")) - 1\n",
    "    imageLabels.append(labelRead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use OpenCV for pre-processing of the image dataset:\n",
    "\n",
    "Here, we are going to scale the image dataset to a resolution of _150 x 150_. We use the most popular face detection tool provided by OpenCV, **haarcascade_frontalface_default.xml**. This classifier is used for effective face detection and the classifier has been trained to extract the useful features from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "faceDetectClassifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imageDataFin = []\n",
    "for i in imageData:\n",
    "    facePoints = faceDetectClassifier.detectMultiScale(i)\n",
    "    x,y = facePoints[0][:2]\n",
    "    cropped = i[y: y + 150, x: x + 150]\n",
    "    imageDataFin.append(cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165, 150, 150)\n"
     ]
    }
   ],
   "source": [
    "c = np.array(imageDataFin)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Set Construction\n",
    "\n",
    "We are going to use the in-built function, *train_test_split*, to split the dataset into training set and testing set as per our defined ratio. The shapes of each are shown here: <br>\n",
    "    <center> <b>X_train:</b> (no_of_examples_in_training_set, width x height) </center>\n",
    "    <center> <b>X_test:</b> (no_of_examples_in_test_set, width x height) </center><br>\n",
    "\n",
    "As per the dataset, we have 15 different subjects in the dataset. Thus the number of classes defined are 15. We can add people or remove as and when we want. <br>\n",
    "Using _np.utils_ we have have defined the classes of each image collected from **y_train**.<br>\n",
    "**X_train** and **X_test** are reshaped into the desired number to ensure there's no discrepancy. We set the type as 'float32' and scale it down to facilitate in the processing step.\n",
    "\n",
    ">#### Why do we need to reshape and scale the training set?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(imageDataFin),np.array(imageLabels), train_size=0.9, test_size=0.1, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (148, 150, 150)\n",
      "X_test shape: (17, 150, 150)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classes = 15\n",
    "y_train = np.array(y_train) \n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((148, 150*150))\n",
    "X_test = X_test.reshape((17, 150*150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training matrix shape (148, 22500)\n",
      "Testing matrix shape (17, 22500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming the neural network using Keras\n",
    "\n",
    "We are using a **3-layer neural network** with self-defined hidden units and activation function. \n",
    "\n",
    "For the first hidden layer, we are using 512 nodes and the activation function used is _relu_. The output of the first hidden layer gets fed into the second hidden layer which has same number of hidden units and activation function.  \n",
    "\n",
    "The third layer or the output layer has the number of nodes equal to the _number of classes_ defined in the dataset. The third layer uses **softmax** as a activation function as the dataset is a multi-class classification problem. \n",
    "\n",
    "\n",
    ">#### What is model and Sequential()?\n",
    ">\n",
    "\n",
    "We have used **dropout** as the regularization technique.\n",
    ">#### What is regularization and dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512,input_shape=(X_train.shape[1],)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               11520512  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15)                7695      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15)                0         \n",
      "=================================================================\n",
      "Total params: 11,790,863\n",
      "Trainable params: 11,790,863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily calculate the number of parameters for each of the layers. \n",
    "> **First Layer**: weights + bias = 22500 x 512 + 512 = 11520512 <br>\n",
    "> **Second Layer**: weights + bias = 512 x 512 + 512 = 262656 <br>\n",
    "> **Third Layer**: weights + bias = 512 x 15 + 15 = 7695 <br>\n",
    "\n",
    "### Running the Model\n",
    "\n",
    "We have hand-picked the hyper-parameters like the _batch-size_ and  _number of epochs_ based on the most optinum combination. The optimizer selected here is **AdamOptimizer**. <br>\n",
    "We define the loss function as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 148 samples, validate on 17 samples\n",
      "Epoch 1/50\n",
      "148/148 [==============================] - 2s 12ms/step - loss: 7.3626 - acc: 0.0743 - val_loss: 10.5325 - val_acc: 0.1176\n",
      "Epoch 2/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 12.5365 - acc: 0.1081 - val_loss: 11.1299 - val_acc: 0.1765\n",
      "Epoch 3/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 12.2671 - acc: 0.1216 - val_loss: 9.0499 - val_acc: 0.2353\n",
      "Epoch 4/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 11.8310 - acc: 0.1757 - val_loss: 9.0031 - val_acc: 0.1765\n",
      "Epoch 5/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 12.1741 - acc: 0.1689 - val_loss: 7.1914 - val_acc: 0.3529\n",
      "Epoch 6/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 12.2930 - acc: 0.1486 - val_loss: 8.1586 - val_acc: 0.3529\n",
      "Epoch 7/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 11.6429 - acc: 0.2297 - val_loss: 10.0574 - val_acc: 0.2941\n",
      "Epoch 8/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 11.7800 - acc: 0.2162 - val_loss: 7.9219 - val_acc: 0.4706\n",
      "Epoch 9/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 11.3569 - acc: 0.2703 - val_loss: 8.3496 - val_acc: 0.4706\n",
      "Epoch 10/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 12.1065 - acc: 0.2297 - val_loss: 8.3520 - val_acc: 0.4706\n",
      "Epoch 11/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 11.8651 - acc: 0.2365 - val_loss: 7.4327 - val_acc: 0.4706\n",
      "Epoch 12/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 11.0311 - acc: 0.2635 - val_loss: 8.5749 - val_acc: 0.4118\n",
      "Epoch 13/50\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 11.3880 - acc: 0.2635 - val_loss: 7.8046 - val_acc: 0.4706\n",
      "Epoch 14/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 11.5432 - acc: 0.2635 - val_loss: 8.1279 - val_acc: 0.4118\n",
      "Epoch 15/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 11.9533 - acc: 0.2230 - val_loss: 6.9210 - val_acc: 0.5294\n",
      "Epoch 16/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 10.3965 - acc: 0.3446 - val_loss: 7.3367 - val_acc: 0.5294\n",
      "Epoch 17/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 10.7191 - acc: 0.2905 - val_loss: 7.0322 - val_acc: 0.4706\n",
      "Epoch 18/50\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 10.4461 - acc: 0.3176 - val_loss: 5.6093 - val_acc: 0.6471\n",
      "Epoch 19/50\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 10.0968 - acc: 0.3243 - val_loss: 5.9511 - val_acc: 0.5294\n",
      "Epoch 20/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.6480 - acc: 0.3446 - val_loss: 6.6370 - val_acc: 0.5294\n",
      "Epoch 21/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.6638 - acc: 0.3514 - val_loss: 5.6570 - val_acc: 0.5882\n",
      "Epoch 22/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.5837 - acc: 0.3378 - val_loss: 5.8428 - val_acc: 0.5882\n",
      "Epoch 23/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.3337 - acc: 0.3851 - val_loss: 6.3675 - val_acc: 0.4706\n",
      "Epoch 24/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.4006 - acc: 0.3716 - val_loss: 5.4595 - val_acc: 0.5294\n",
      "Epoch 25/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.6974 - acc: 0.4054 - val_loss: 6.4865 - val_acc: 0.3529\n",
      "Epoch 26/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.0439 - acc: 0.3311 - val_loss: 6.3451 - val_acc: 0.3529\n",
      "Epoch 27/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.2304 - acc: 0.4257 - val_loss: 4.9143 - val_acc: 0.5294\n",
      "Epoch 28/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 7.8965 - acc: 0.4257 - val_loss: 3.6284 - val_acc: 0.5294\n",
      "Epoch 29/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 5.4536 - acc: 0.4662 - val_loss: 5.6505 - val_acc: 0.3529\n",
      "Epoch 30/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 5.0665 - acc: 0.4459 - val_loss: 3.3684 - val_acc: 0.5294\n",
      "Epoch 31/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 3.7471 - acc: 0.4730 - val_loss: 1.4180 - val_acc: 0.7059\n",
      "Epoch 32/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.6736 - acc: 0.5473 - val_loss: 1.1415 - val_acc: 0.5882\n",
      "Epoch 33/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3142 - acc: 0.5676 - val_loss: 1.2875 - val_acc: 0.5294\n",
      "Epoch 34/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0930 - acc: 0.5946 - val_loss: 1.0733 - val_acc: 0.7059\n",
      "Epoch 35/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0211 - acc: 0.6554 - val_loss: 0.8356 - val_acc: 0.7647\n",
      "Epoch 36/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.8965 - acc: 0.7027 - val_loss: 0.6311 - val_acc: 0.8235\n",
      "Epoch 37/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.6995 - acc: 0.7297 - val_loss: 0.6216 - val_acc: 0.7647\n",
      "Epoch 38/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.5356 - acc: 0.8378 - val_loss: 0.5708 - val_acc: 0.8235\n",
      "Epoch 39/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.4641 - acc: 0.8716 - val_loss: 0.5437 - val_acc: 0.8235\n",
      "Epoch 40/50\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.4469 - acc: 0.8311 - val_loss: 0.5627 - val_acc: 0.8235\n",
      "Epoch 41/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 0.4633 - acc: 0.8581 - val_loss: 0.6022 - val_acc: 0.8824\n",
      "Epoch 42/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.3265 - acc: 0.9122 - val_loss: 0.5157 - val_acc: 0.9412\n",
      "Epoch 43/50\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 0.3250 - acc: 0.9054 - val_loss: 0.4343 - val_acc: 0.8824\n",
      "Epoch 44/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.2873 - acc: 0.8919 - val_loss: 0.3981 - val_acc: 0.8824\n",
      "Epoch 45/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.2645 - acc: 0.8986 - val_loss: 0.3980 - val_acc: 0.8235\n",
      "Epoch 46/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.2558 - acc: 0.9189 - val_loss: 0.3296 - val_acc: 0.8824\n",
      "Epoch 47/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.1644 - acc: 0.9595 - val_loss: 0.2973 - val_acc: 0.9412\n",
      "Epoch 48/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.1587 - acc: 0.9527 - val_loss: 0.3003 - val_acc: 0.9412\n",
      "Epoch 49/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.2246 - acc: 0.9189 - val_loss: 0.2316 - val_acc: 0.9412\n",
      "Epoch 50/50\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.1282 - acc: 0.9527 - val_loss: 0.2152 - val_acc: 0.9412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f629ffff98>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=64, epochs=50, verbose=1, validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the performance of the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test,Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21521124243736267"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94117647409439087"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "17/17 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_classes= model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_classified_indices = np.nonzero(predicted_classes == y_test)[0]\n",
    "incorrect_classified_indices = np.nonzero(predicted_classes != y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_classified_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_classified_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
